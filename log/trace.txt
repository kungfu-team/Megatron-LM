{ // main
    { // model_provider
    } // model_provider | 0
isfile(/root/var/ckpt/bert/latest_checkpointed_iteration.txt)
    { // train_valid_test_datasets_provider
train_valid_test_datasets_provider([800, 80, 80])
        { // get_samples_mapping
isfile(/data/megatron-lm/bert/enwiki/bert_text_sentence_train_indexmap_800mns_509msl_0.10ssp_1234s.npy)
        } // get_samples_mapping | 0
        { // get_samples_mapping
isfile(/data/megatron-lm/bert/enwiki/bert_text_sentence_valid_indexmap_80mns_509msl_0.10ssp_1234s.npy)
        } // get_samples_mapping | 1
        { // get_samples_mapping
isfile(/data/megatron-lm/bert/enwiki/bert_text_sentence_test_indexmap_80mns_509msl_0.10ssp_1234s.npy)
        } // get_samples_mapping | 2
    } // train_valid_test_datasets_provider | 0
    { // forward_step
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
        { // get_batch
        } // get_batch | 0
    } // forward_step | 0
    { // loss_func
    } // loss_func | 0
    { // forward_step
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
        { // get_batch
        } // get_batch | 1
    } // forward_step | 1
    { // loss_func
    } // loss_func | 1
    { // forward_step
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
        { // get_batch
        } // get_batch | 2
    } // forward_step | 2
    { // loss_func
    } // loss_func | 2
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d9b040>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8d99180>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
forward_step | [0]: <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fdbf8da32b0>
forward_step | [1]: DistributedDataParallel(
  (module): Float16Module(
    (module): BertModel(
      (language_model): TransformerLanguageModel(
        (embedding): Embedding(
          (word_embeddings): VocabParallelEmbedding()
          (position_embeddings): Embedding(512, 1024)
          (tokentype_embeddings): Embedding(2, 1024)
          (embedding_dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): ParallelTransformer(
          (layers): ModuleList(
            (0-23): 24 x ParallelTransformerLayer(
              (input_layernorm): MixedFusedLayerNorm()
              (self_attention): ParallelAttention(
                (query_key_value): ColumnParallelLinear()
                (core_attention): CoreAttention(
                  (scale_mask_softmax): FusedScaleMaskSoftmax()
                  (attention_dropout): Dropout(p=0.1, inplace=False)
                )
                (dense): RowParallelLinear()
              )
              (post_attention_layernorm): MixedFusedLayerNorm()
              (mlp): ParallelMLP(
                (dense_h_to_4h): ColumnParallelLinear()
                (dense_4h_to_h): RowParallelLinear()
              )
            )
          )
          (final_layernorm): MixedFusedLayerNorm()
        )
        (pooler): Pooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (lm_head): BertLMHead(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (layernorm): MixedFusedLayerNorm()
      )
      (binary_head): Linear(in_features=1024, out_features=2, bias=True)
    )
  )
)
} // main | 0
